# ğŸ§  Multi-Modal Physical Exercise Classification

> ğŸ”¬ Master's Course Project â€” **Multi-Modal Data Fusion**  
> ğŸ“ University of Oulu

---

## ğŸ‹ï¸â€â™‚ï¸ Overview

This project explores the use of **multi-modal sensor data** for recognizing physical exercises. It combines **wearable accelerometer data** and **depth camera recordings** to classify different body movements.

The primary goal is to develop a **user-independent machine learning model** that can recognize exercises performed by individuals, even if the model has not seen data from those individuals before.

---

## ğŸ“¦ Dataset

We use the publicly available [**MEx Dataset**](https://archive.ics.uci.edu/dataset/500/mex), which contains sensor readings from **30 participants** performing a variety of physical exercises. For this project, a **subset of 10 participants** is used.

### ğŸ” Modalities:
- **Accelerometer** data (placed on the thigh)
- **Depth camera** data (for capturing skeletal/body movement)

### âš™ï¸ Key Features:
- Multi-modal time-series data
- Designed for **user-independent classification**
- Lying-down exercise context (on a mat)

---

## ğŸ§  Methods & Techniques

- ğŸ“Š **Data Preprocessing**
  - Normalization, synchronization, and filtering
  - Handling missing data from depth sensors

- ğŸ§± **Feature Engineering**
  - Temporal and frequency domain features
  - Joint movement and acceleration patterns

- ğŸ§ª **Modeling**
  - Classical ML: Random Forests, SVM, k-NN
  - Deep Learning: LSTM / CNN for sequence modeling
  - Fusion strategies: Early fusion, late fusion, hybrid

- ğŸ“ˆ **Evaluation**
  - Train-test split across users (user-independent)
  - Accuracy, precision, recall, F1-score

---

